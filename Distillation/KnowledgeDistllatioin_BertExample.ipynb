{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b075d1bb",
   "metadata": {},
   "source": [
    "### Overview\n",
    "- This notebook shows an example on how to perform Knowledge distillation of Bert Model to a smaller model using KL divergence loss. \n",
    "- An example phishing dataset is used for demo\n",
    "- These are the following steps:\n",
    "    0. We load the dataset\n",
    "    1. We train a teacher BERT model on this dataset\n",
    "    2. We train a smaller student BERT model using KL divergence loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c201d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d73223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052cf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2825800",
   "metadata": {},
   "source": [
    "### 0. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08cf3e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "dataset_dict = load_dataset(\"imdb\")\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b51b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__'],\n",
       "     num_rows: 22500\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__'],\n",
       "     num_rows: 2500\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split train into train & dev\n",
    "train_dataset = dataset_dict['train']\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "X = train_df.drop('label', axis=1) \n",
    "y = train_df['label']\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(pd.concat([X_train, y_train], axis=1))\n",
    "dev_dataset = Dataset.from_pandas(pd.concat([X_dev, y_dev], axis=1))\n",
    "test_dataset = dataset_dict['test']\n",
    "\n",
    "train_dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf6e581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['\"Algie, the Miner\" is one bad and unfunny silent comedy. The timing of the slapstick is completely off. This is the kind of humor with certain sequences that make you wonder if they\\'re supposed to be funny or not. However, the actual quality of the film is irrelevant. This is mandatory viewing for film buffs mainly because its one of the earliest examples of gay cinema. The main character of Algie is an effeminate guy, acting much like the stereotypical \"pansy\" common in many early films. The film has the homophobic attitude common of the time. \"Algie, the Miner\" is pretty awful, but fascinating from a historical viewpoint. (3/10)',\n",
       "  'This is a complete Hoax...<br /><br />The movie clearly has been shot in north western Indian state of Rajasthan. Look at the chase scene - the vehicles are Indian; the writing all over is Hindi - language used in India. The drive through is on typical Jaipur streets. Also the palace is in Amer - about 10 miles from Jaipur, Rajasthan. The film-makers in their (about the film) in DVD Bonus seem to make it sound that they risked their lives shooting in Kabul and around. Almost all of their action scenes are shot in India. The scene where they see a group singing around fire is so fake that they did not even think about changing it to Afgani folk song. They just recorded the Rajasthani folk song. How do I know it because I have traveled that area extensively. They are just on the band-wagon to make big on the issue. I do challenge the film makers to deny it.',\n",
       "  \"Nifty little episode played mainly for laughs, but with clever dollop of suspense. Somehow a Martian has snuck aboard a broken-down bus on its way to nowhere, but which passenger is it, (talk about your illegal immigrants!). All-star supporting cast, from wild-eyed Jack Elam (hamming it up shamelessly), to sexy Jean Willes (if she's the Martian, then I say let's open the borders!), to cruel-faced John Hoyt (the most obvious suspect), along with familiar faces John Archer and Barney Phillips (and a nice turn from Bill Kendis as the bus driver). Makes for a very entertaining half-hour even if the action is confined to a single set.\",\n",
       "  \"Perhaps the worst thing about Carlos Mencia's comedy is that every joke needs to be followed with an insult at the people in the crowd that aren't laughing. If there's anybody who's insecure, it's a comedian who won't shut up about his audience.<br /><br />Then again, perhaps the worst thing about Carlos Mencia's comedy is that he doesn't get off his free speech high horse. If you want to be funny, just make a joke, don't explain all the reasons why you're saving the American way with your failed attempts at generating laughter.<br /><br />Hmm... actually... the worst thing about Carlos Mencia's comedy is that it substitues meanspirited jabs at ethnicities for legitimate humor. Avoid this like the plague.\",\n",
       "  \"First of all, I firmly believe that Norwegian movies are continually getting better. From the tedious emotional films of the 70's and 80's, movies from this place actually started to contain a bit of humour. Imagine.. Actual comedies were made! Movies were actually starting to get entertaining and funny, as opposed to long, dark, depressing and boring.<br /><br />During the 90's and 00's several really great movies were made by a 'new generation' of filmmakers. Movie after movie were praised by critics and played loads of money. It became the norm!<br /><br />Then came United...<br /><br />*MINOR SPOILERS* It's just simply not funny. Not once. Not ever. But the thing is... We THINK its funny. Because we're used to norwegian movies to be funny. Especially with a cast like this with a few really funny comedians. But.. They neither say nor do anything funny! Where's the humor? Show me the humor! Is it the awkward clerk played by Harald Eia? Is it the overacting totally ridiculously unrealistic football coach? Is it the commentaries by Arne Scheie? The movie is just not funny!<br /><br />But thats not my main rant about United. That namely is the predictability. (And it is here I fear that norwegian comedies have come to a standstill since I have seen this in many other movies as well.) All the time you just know its going to end well. All characters are exactly as they are presented in the start of the movie, and everybody gets exactly what they deserve in the end. There's absolutely no room for surprises at all!<br /><br />All in all I can say that I sat with a bad feeling after seeing this movie. It was the one movie that made me realize that we probably need some new blood in norwegian movie making... again!<br /><br />Rating: 1/6\"],\n",
       " 'label': [0, 0, 1, 0, 0],\n",
       " '__index_level_0__': [6740, 8635, 17340, 2303, 6014]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View examples\n",
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca1996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally select only few training examples to verify functionality of the code\n",
    "if 0:\n",
    "    train_dataset = train_dataset.select(range(100))\n",
    "    dev_dataset = dev_dataset.select(range(50))\n",
    "    test_dataset = test_dataset.select(range(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc77051",
   "metadata": {},
   "source": [
    "### 1. Train teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e07e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                           num_labels=2, \n",
    "                                                           id2label=id2label, \n",
    "                                                           label2id=label2id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f305a8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's number of parameters: 109,483,778\n"
     ]
    }
   ],
   "source": [
    "# Num of model params\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model's number of parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ea7627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight True\n",
      "bert.embeddings.position_embeddings.weight True\n",
      "bert.embeddings.token_type_embeddings.weight True\n",
      "bert.embeddings.LayerNorm.weight True\n",
      "bert.embeddings.LayerNorm.bias True\n",
      "bert.encoder.layer.0.attention.self.query.weight False\n",
      "bert.encoder.layer.0.attention.self.query.bias False\n",
      "bert.encoder.layer.0.attention.self.key.weight False\n",
      "bert.encoder.layer.0.attention.self.key.bias False\n",
      "bert.encoder.layer.0.attention.self.value.weight False\n",
      "bert.encoder.layer.0.attention.self.value.bias False\n",
      "bert.encoder.layer.0.attention.output.dense.weight False\n",
      "bert.encoder.layer.0.attention.output.dense.bias False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.0.intermediate.dense.weight False\n",
      "bert.encoder.layer.0.intermediate.dense.bias False\n",
      "bert.encoder.layer.0.output.dense.weight False\n",
      "bert.encoder.layer.0.output.dense.bias False\n",
      "bert.encoder.layer.0.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.attention.self.query.weight False\n",
      "bert.encoder.layer.1.attention.self.query.bias False\n",
      "bert.encoder.layer.1.attention.self.key.weight False\n",
      "bert.encoder.layer.1.attention.self.key.bias False\n",
      "bert.encoder.layer.1.attention.self.value.weight False\n",
      "bert.encoder.layer.1.attention.self.value.bias False\n",
      "bert.encoder.layer.1.attention.output.dense.weight False\n",
      "bert.encoder.layer.1.attention.output.dense.bias False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.intermediate.dense.weight False\n",
      "bert.encoder.layer.1.intermediate.dense.bias False\n",
      "bert.encoder.layer.1.output.dense.weight False\n",
      "bert.encoder.layer.1.output.dense.bias False\n",
      "bert.encoder.layer.1.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.attention.self.query.weight False\n",
      "bert.encoder.layer.2.attention.self.query.bias False\n",
      "bert.encoder.layer.2.attention.self.key.weight False\n",
      "bert.encoder.layer.2.attention.self.key.bias False\n",
      "bert.encoder.layer.2.attention.self.value.weight False\n",
      "bert.encoder.layer.2.attention.self.value.bias False\n",
      "bert.encoder.layer.2.attention.output.dense.weight False\n",
      "bert.encoder.layer.2.attention.output.dense.bias False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.intermediate.dense.weight False\n",
      "bert.encoder.layer.2.intermediate.dense.bias False\n",
      "bert.encoder.layer.2.output.dense.weight False\n",
      "bert.encoder.layer.2.output.dense.bias False\n",
      "bert.encoder.layer.2.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.attention.self.query.weight False\n",
      "bert.encoder.layer.3.attention.self.query.bias False\n",
      "bert.encoder.layer.3.attention.self.key.weight False\n",
      "bert.encoder.layer.3.attention.self.key.bias False\n",
      "bert.encoder.layer.3.attention.self.value.weight False\n",
      "bert.encoder.layer.3.attention.self.value.bias False\n",
      "bert.encoder.layer.3.attention.output.dense.weight False\n",
      "bert.encoder.layer.3.attention.output.dense.bias False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.intermediate.dense.weight False\n",
      "bert.encoder.layer.3.intermediate.dense.bias False\n",
      "bert.encoder.layer.3.output.dense.weight False\n",
      "bert.encoder.layer.3.output.dense.bias False\n",
      "bert.encoder.layer.3.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.attention.self.query.weight False\n",
      "bert.encoder.layer.4.attention.self.query.bias False\n",
      "bert.encoder.layer.4.attention.self.key.weight False\n",
      "bert.encoder.layer.4.attention.self.key.bias False\n",
      "bert.encoder.layer.4.attention.self.value.weight False\n",
      "bert.encoder.layer.4.attention.self.value.bias False\n",
      "bert.encoder.layer.4.attention.output.dense.weight False\n",
      "bert.encoder.layer.4.attention.output.dense.bias False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.intermediate.dense.weight False\n",
      "bert.encoder.layer.4.intermediate.dense.bias False\n",
      "bert.encoder.layer.4.output.dense.weight False\n",
      "bert.encoder.layer.4.output.dense.bias False\n",
      "bert.encoder.layer.4.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.attention.self.query.weight False\n",
      "bert.encoder.layer.5.attention.self.query.bias False\n",
      "bert.encoder.layer.5.attention.self.key.weight False\n",
      "bert.encoder.layer.5.attention.self.key.bias False\n",
      "bert.encoder.layer.5.attention.self.value.weight False\n",
      "bert.encoder.layer.5.attention.self.value.bias False\n",
      "bert.encoder.layer.5.attention.output.dense.weight False\n",
      "bert.encoder.layer.5.attention.output.dense.bias False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.intermediate.dense.weight False\n",
      "bert.encoder.layer.5.intermediate.dense.bias False\n",
      "bert.encoder.layer.5.output.dense.weight False\n",
      "bert.encoder.layer.5.output.dense.bias False\n",
      "bert.encoder.layer.5.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.attention.self.query.weight False\n",
      "bert.encoder.layer.6.attention.self.query.bias False\n",
      "bert.encoder.layer.6.attention.self.key.weight False\n",
      "bert.encoder.layer.6.attention.self.key.bias False\n",
      "bert.encoder.layer.6.attention.self.value.weight False\n",
      "bert.encoder.layer.6.attention.self.value.bias False\n",
      "bert.encoder.layer.6.attention.output.dense.weight False\n",
      "bert.encoder.layer.6.attention.output.dense.bias False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.intermediate.dense.weight False\n",
      "bert.encoder.layer.6.intermediate.dense.bias False\n",
      "bert.encoder.layer.6.output.dense.weight False\n",
      "bert.encoder.layer.6.output.dense.bias False\n",
      "bert.encoder.layer.6.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.attention.self.query.weight False\n",
      "bert.encoder.layer.7.attention.self.query.bias False\n",
      "bert.encoder.layer.7.attention.self.key.weight False\n",
      "bert.encoder.layer.7.attention.self.key.bias False\n",
      "bert.encoder.layer.7.attention.self.value.weight False\n",
      "bert.encoder.layer.7.attention.self.value.bias False\n",
      "bert.encoder.layer.7.attention.output.dense.weight False\n",
      "bert.encoder.layer.7.attention.output.dense.bias False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.intermediate.dense.weight False\n",
      "bert.encoder.layer.7.intermediate.dense.bias False\n",
      "bert.encoder.layer.7.output.dense.weight False\n",
      "bert.encoder.layer.7.output.dense.bias False\n",
      "bert.encoder.layer.7.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.attention.self.query.weight False\n",
      "bert.encoder.layer.8.attention.self.query.bias False\n",
      "bert.encoder.layer.8.attention.self.key.weight False\n",
      "bert.encoder.layer.8.attention.self.key.bias False\n",
      "bert.encoder.layer.8.attention.self.value.weight False\n",
      "bert.encoder.layer.8.attention.self.value.bias False\n",
      "bert.encoder.layer.8.attention.output.dense.weight False\n",
      "bert.encoder.layer.8.attention.output.dense.bias False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.intermediate.dense.weight False\n",
      "bert.encoder.layer.8.intermediate.dense.bias False\n",
      "bert.encoder.layer.8.output.dense.weight False\n",
      "bert.encoder.layer.8.output.dense.bias False\n",
      "bert.encoder.layer.8.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.attention.self.query.weight False\n",
      "bert.encoder.layer.9.attention.self.query.bias False\n",
      "bert.encoder.layer.9.attention.self.key.weight False\n",
      "bert.encoder.layer.9.attention.self.key.bias False\n",
      "bert.encoder.layer.9.attention.self.value.weight False\n",
      "bert.encoder.layer.9.attention.self.value.bias False\n",
      "bert.encoder.layer.9.attention.output.dense.weight False\n",
      "bert.encoder.layer.9.attention.output.dense.bias False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.intermediate.dense.weight False\n",
      "bert.encoder.layer.9.intermediate.dense.bias False\n",
      "bert.encoder.layer.9.output.dense.weight False\n",
      "bert.encoder.layer.9.output.dense.bias False\n",
      "bert.encoder.layer.9.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.attention.self.query.weight False\n",
      "bert.encoder.layer.10.attention.self.query.bias False\n",
      "bert.encoder.layer.10.attention.self.key.weight False\n",
      "bert.encoder.layer.10.attention.self.key.bias False\n",
      "bert.encoder.layer.10.attention.self.value.weight False\n",
      "bert.encoder.layer.10.attention.self.value.bias False\n",
      "bert.encoder.layer.10.attention.output.dense.weight False\n",
      "bert.encoder.layer.10.attention.output.dense.bias False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.intermediate.dense.weight False\n",
      "bert.encoder.layer.10.intermediate.dense.bias False\n",
      "bert.encoder.layer.10.output.dense.weight False\n",
      "bert.encoder.layer.10.output.dense.bias False\n",
      "bert.encoder.layer.10.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.attention.self.query.weight False\n",
      "bert.encoder.layer.11.attention.self.query.bias False\n",
      "bert.encoder.layer.11.attention.self.key.weight False\n",
      "bert.encoder.layer.11.attention.self.key.bias False\n",
      "bert.encoder.layer.11.attention.self.value.weight False\n",
      "bert.encoder.layer.11.attention.self.value.bias False\n",
      "bert.encoder.layer.11.attention.output.dense.weight False\n",
      "bert.encoder.layer.11.attention.output.dense.bias False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.intermediate.dense.weight False\n",
      "bert.encoder.layer.11.intermediate.dense.bias False\n",
      "bert.encoder.layer.11.output.dense.weight False\n",
      "bert.encoder.layer.11.output.dense.bias False\n",
      "bert.encoder.layer.11.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.output.LayerNorm.bias False\n",
      "bert.pooler.dense.weight False\n",
      "bert.pooler.dense.bias False\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "# Only train classifier layer. Freeze remaining params\n",
    "for name, param in model.named_parameters():\n",
    "    if (\"encoder\" in name) or (\"pooler\" in name):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# print layers\n",
    "for name, param in model.named_parameters():\n",
    "   print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a3c9c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22500/22500 [00:15<00:00, 1417.61 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:01<00:00, 1474.30 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:17<00:00, 1469.24 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 22500\n",
       " }),\n",
       " 22500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer & Preprocess text\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "\n",
    "train_tokenized_data = train_dataset.map(preprocess_function, batched=True)\n",
    "dev_tokenized_data = dev_dataset.map(preprocess_function, batched=True)\n",
    "test_tokenized_data = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_tokenized_data, len(train_tokenized_data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2654d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics function\n",
    "eval_acc = evaluate.load(\"accuracy\")\n",
    "eval_roc_auc = evaluate.load(\"roc_auc\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred    # shape: (num_examples, num_classes), (num_examples, )\n",
    "    \n",
    "    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
    "    positive_class_probs = probabilities[:, 1]\n",
    "    roc_auc = eval_roc_auc.compute(prediction_scores=positive_class_probs, references=labels)['roc_auc']\n",
    "    \n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    accuracy = eval_acc.compute(predictions=preds, references=labels)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"roc_auc\": roc_auc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5a75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "# Transforms list of dicts to dict of lists (batch)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cfce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1056' max='1056' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1056/1056 06:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.433800</td>\n",
       "      <td>0.361541</td>\n",
       "      <td>{'accuracy': 0.856}</td>\n",
       "      <td>0.953689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.344137</td>\n",
       "      <td>{'accuracy': 0.8712}</td>\n",
       "      <td>0.958824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.385555</td>\n",
       "      <td>{'accuracy': 0.8772}</td>\n",
       "      <td>0.959486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1056, training_loss=0.24787978692488236, metrics={'train_runtime': 386.9079, 'train_samples_per_second': 174.46, 'train_steps_per_second': 2.729, 'total_flos': 1.77599962368e+16, 'train_loss': 0.24787978692488236, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_epochs=3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_dir/bert_imdb_teacher_model\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_num_workers=8\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_data,\n",
    "    eval_dataset=dev_tokenized_data,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ba83be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': {'accuracy': 0.86472}, 'roc_auc': 0.9529937472000001}\n",
      "Teacher model:\n",
      "Test set - Accuracy: 0.8647, Precision: 0.8120, Recall: 0.9492, F1 Score: 0.8753\n"
     ]
    }
   ],
   "source": [
    "# Score on test set\n",
    "\n",
    "predictions = trainer.predict(test_tokenized_data)\n",
    "logits, labels = predictions.predictions, predictions.label_ids\n",
    "metrics = compute_metrics((logits, labels))\n",
    "print(metrics)\n",
    "\n",
    "# More eval metrics\n",
    "preds = np.argmax(logits, axis=1)\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "print(f\"Teacher model:\") \n",
    "print(f\"Test set - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdcabb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.', 'label': 0}\n",
      "GT label: NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# Select a test example\n",
    "print(test_dataset[0])\n",
    "print(\"GT label:\", id2label[test_dataset[0]['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5ad2cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "# Run Inference on this test example\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "input_text = test_dataset[0]['text']\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "pred = model.config.id2label[preds.item()]\n",
    "print(f\"Model prediction: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd961c5",
   "metadata": {},
   "source": [
    "### 2. Train student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "deb99ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher model and tokenizer\n",
    "model_path = \"output_dir/bert_imdb_teacher_model/checkpoint-1056\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c880e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load student model\n",
    "small_bert_config = DistilBertConfig(n_heads=8, n_layers=1)\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                                config=small_bert_config,).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "599b4b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model number of parameters: 109,483,778\n",
      "Student model number of parameters: 31,515,650\n"
     ]
    }
   ],
   "source": [
    "# Num of model params\n",
    "num_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(f\"Teacher model number of parameters: {num_params:,}\")\n",
    "\n",
    "num_params = sum(p.numel() for p in student_model.parameters())\n",
    "print(f\"Student model number of parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5f06808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight True\n",
      "distilbert.embeddings.position_embeddings.weight True\n",
      "distilbert.embeddings.LayerNorm.weight True\n",
      "distilbert.embeddings.LayerNorm.bias True\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight True\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias True\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight True\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias True\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight True\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias True\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight True\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias True\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight True\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias True\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight True\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias True\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight True\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias True\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight True\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias True\n",
      "pre_classifier.weight True\n",
      "pre_classifier.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "# # Only train classifier layer. Freeze remaining params\n",
    "# for name, param in student_model.named_parameters():\n",
    "#     if (\"distilbert\" in name) or (\"pre_classifier\" in name):\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# # Number of trainable params\n",
    "# num_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
    "# print(f\"Student model number of trainable parameters: {num_params:,}\")\n",
    "\n",
    "# print layers\n",
    "for name, param in student_model.named_parameters():\n",
    "   print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f468168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/22500 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22500/22500 [00:15<00:00, 1443.13 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:01<00:00, 1438.94 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:17<00:00, 1458.88 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 22500\n",
       " }),\n",
       " 22500)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer & Preprocess text\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "\n",
    "train_tokenized_data = train_dataset.map(preprocess_function, batched=True)\n",
    "dev_tokenized_data = dev_dataset.map(preprocess_function, batched=True)\n",
    "test_tokenized_data = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_tokenized_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "dev_tokenized_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_tokenized_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_tokenized_data, len(train_tokenized_data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5200400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 40 391\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# create training data loader\n",
    "train_dataloader = DataLoader(train_tokenized_data, batch_size=batch_size, num_workers=8)\n",
    "# create dev data loader\n",
    "dev_dataloader = DataLoader(dev_tokenized_data, batch_size=batch_size, num_workers=8)\n",
    "# create testing data loader\n",
    "test_dataloader = DataLoader(test_tokenized_data, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "print(len(train_dataloader), len(dev_dataloader), len(test_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f72243d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cf74a92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher:\n",
      "Dev set - Accuracy: 0.8772, Precision: 0.8327, Recall: 0.9440, F1 Score: 0.8849\n",
      "Test set - Accuracy: 0.8684, Precision: 0.8187, Recall: 0.9464, F1 Score: 0.8780\n"
     ]
    }
   ],
   "source": [
    "# Teacher model - dev & test set scores\n",
    "\n",
    "print(f\"Teacher:\")\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_model(teacher_model, dev_dataloader, device)\n",
    "print(f\"Dev set - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_model(teacher_model, test_dataloader, device)\n",
    "print(f\"Test set - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac262fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student (before training):\n",
      "Dev set - Accuracy: 0.5248, Precision: 0.5143, Recall: 0.8936, F1 Score: 0.6528\n",
      "Test set - Accuracy: 0.5151, Precision: 0.5087, Recall: 0.8798, F1 Score: 0.6447\n"
     ]
    }
   ],
   "source": [
    "# Student model - before training - dev & test set scores\n",
    "\n",
    "print(f\"Student (before training):\")\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_model(student_model, dev_dataloader, device)\n",
    "print(f\"Dev set - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_model(student_model, test_dataloader, device)\n",
    "print(f\"Test set - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f48c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 batches\n",
      "Processed 100 batches\n",
      "Processed 150 batches\n",
      "Processed 200 batches\n",
      "Processed 250 batches\n",
      "Processed 300 batches\n",
      "Processed 350 batches\n",
      "Epoch 1 completed with loss: 0.33669546246528625\n",
      "Student Dev set - Accuracy: 0.8932, Precision: 0.9032, Recall: 0.8808, F1 Score: 0.8919\n",
      "Processed 50 batches\n",
      "Processed 100 batches\n",
      "Processed 150 batches\n",
      "Processed 200 batches\n",
      "Processed 250 batches\n",
      "Processed 300 batches\n",
      "Processed 350 batches\n",
      "Epoch 2 completed with loss: 0.15537892282009125\n",
      "Student Dev set - Accuracy: 0.8880, Precision: 0.8719, Recall: 0.9096, F1 Score: 0.8904\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "num_epochs = 2\n",
    "T = 2.0 # temperature\n",
    "alpha = 0.5\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "\n",
    "# Knowledge Distillation loss (KL div loss + CE loss)\n",
    "def knowledge_distillation_loss(student_logits, teacher_logits, true_labels, T=2.0, alpha=0.5):    \n",
    "    # KL Divergence loss for distillation\n",
    "    teacher_probs = nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "    student_log_probs = nn.functional.log_softmax(student_logits / T, dim=1)\n",
    "    distill_loss = nn.functional.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (T ** 2)\n",
    "    # Cross-entropy loss for true labels\n",
    "    ce_loss = nn.CrossEntropyLoss()(student_logits, true_labels)\n",
    "    # Total loss\n",
    "    loss = alpha * distill_loss + (1.0 - alpha) * ce_loss\n",
    "    return loss\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_ix, batch in enumerate(train_dataloader):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Teacher model\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # Student model\n",
    "        student_outputs = student_model(input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        # Loss + Backprop\n",
    "        loss = knowledge_distillation_loss(student_logits, teacher_logits, labels, T, alpha)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_ix + 1) % 50 == 0:\n",
    "            print(f\"Processed {batch_ix + 1} batches\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed with loss: {loss.item()}\")\n",
    "\n",
    "    # Dev set scores\n",
    "    accuracy, precision, recall, f1 = evaluate_model(student_model, dev_dataloader, device)\n",
    "    print(f\"Student Dev set - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    student_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3b7bbcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student (after training):\n",
      "Test set - Accuracy: 0.8744, Precision: 0.8647, Recall: 0.8876, F1 Score: 0.8760\n"
     ]
    }
   ],
   "source": [
    "# Test set scores\n",
    "print(f\"Student (after training):\")\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_model(student_model, test_dataloader, device)\n",
    "print(f\"Test set - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d31c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13d3e64c",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/model-compression\n",
    "2. https://huggingface.co/docs/transformers/en/tasks/sequence_classification\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
