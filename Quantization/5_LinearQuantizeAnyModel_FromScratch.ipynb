{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "height": 31
   },
   "source": [
    "## Reference:\n",
    "- https://learn.deeplearning.ai/courses/quantization-in-depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview: Custom Build an 8-Bit Quantizer\n",
    "\n",
    "This notebook shows how to:\n",
    "- Build a custom 8-bit quantizer `W8A16LinearLayer()`\n",
    "- Compress any model in 8-bit precision using this.\n",
    "- Replace Pytorch layers with quantized layers\n",
    "- Quantize any Open source Pytorch model\n",
    "- Load quantized weights from Huggingface hub in memory efficient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 65
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "height": 31
   },
   "source": [
    "### Check quantize error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 643
   },
   "outputs": [],
   "source": [
    "def get_q_scale_symmetric(tensor, dtype=torch.int8):\n",
    "    r_max = tensor.abs().max().item()\n",
    "    q_max = torch.iinfo(dtype).max\n",
    "\n",
    "    # return the scale\n",
    "    return r_max/q_max\n",
    "\n",
    "\n",
    "def linear_q_symmetric(tensor, dtype=torch.int8):\n",
    "    scale = get_q_scale_symmetric(tensor)\n",
    "    \n",
    "    quantized_tensor = linear_q_with_scale_and_zero_point(tensor,\n",
    "                                                     scale=scale,\n",
    "                   # in symmetric quantization zero point is = 0    \n",
    "                                                    zero_point=0,\n",
    "                                                      dtype=dtype)\n",
    "    \n",
    "    return quantized_tensor, scale\n",
    "\n",
    "def linear_q_with_scale_and_zero_point(\n",
    "    r_tensor, scale, zero_point, dtype=torch.int8):\n",
    "    \"\"\"\n",
    "    Performs simple linear quantization given\n",
    "    the scale and zero-point.\n",
    "    \"\"\"\n",
    "\n",
    "    # scale tensor and add the zero point\n",
    "    scaled_and_shifted_tensor = r_tensor / scale + zero_point\n",
    "\n",
    "    # round the tensor \n",
    "    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n",
    "\n",
    "    # we need to clamp to the min/max value of the specified dtype\n",
    "    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n",
    "    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n",
    "    return q_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 218
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1628, -0.5933, -2.2189])\n",
      "tensor([-1.1603, -0.5911, -2.2290])\n"
     ]
    }
   ],
   "source": [
    "hs = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "w = torch.randn((3,3))\n",
    "q_w, s_w  = linear_q_symmetric(w)\n",
    "\n",
    "# Using Quantized weight\n",
    "dequantized_weight = q_w.to(torch.float32) * s_w + 0 # z_w\n",
    "output = torch.nn.functional.linear(hs, dequantized_weight)\n",
    "print(output)\n",
    "\n",
    "# Using Original non-quuanized weight\n",
    "output = torch.nn.functional.linear(hs, w)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "height": 31
   },
   "source": [
    "### `w8_a16_forward` Function\n",
    "\n",
    "-\n",
    "```Python\n",
    "W8A16LinearLayer\n",
    "                    # 8-bit  # 16-bit         # optional\n",
    "* w8_a16_forward -> weights, input,   scales, bias=None\n",
    "                    \n",
    "```\n",
    "- Cast the 8-bit `weights` to the same data type as the `input`, \"casted weights\",\n",
    "- keeping the \"casted weights\" in the same range as before, [-128, 127]\n",
    "- Output: $$(({inputs} \\cdot \\text{\"casted weights\"}) * {scale}) + {bias}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 116
   },
   "outputs": [],
   "source": [
    "w = torch.randint(-128, 127, (32, 16))\n",
    "w_int8 = w.to(torch.int8)\n",
    "hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
    "\n",
    "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
    "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 110.0000,  -53.5000, -300.0000,  812.0000,   40.0000,   67.0000,\n",
       "           76.5000,   -7.9375, -164.0000, -122.0000,    7.8125,  -62.2500,\n",
       "          183.0000,  154.0000,    6.0000, -344.0000,  -60.0000,  -13.0000,\n",
       "         -128.0000,   11.1250,   58.7500,  468.0000,  346.0000,   36.0000,\n",
       "            1.3672, -212.0000, -129.0000,  -20.2500, -256.0000,  548.0000,\n",
       "           36.0000,  159.0000]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F.linear(hs, w_int8.to(hs.dtype)) * scales) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 116
   },
   "outputs": [],
   "source": [
    "def w8_a16_forward(weight, input, scales, bias=None):    \n",
    "    casted_weights = weight.to(input.dtype)\n",
    "    output = F.linear(input, casted_weights) * scales\n",
    "    if bias is not None:\n",
    "        output = output + bias      \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 99
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With bias:\n",
      "\n",
      " tensor([[ 110.0000,  -53.5000, -300.0000,  812.0000,   40.0000,   67.0000,\n",
      "           76.5000,   -7.9375, -164.0000, -122.0000,    7.8125,  -62.2500,\n",
      "          183.0000,  154.0000,    6.0000, -344.0000,  -60.0000,  -13.0000,\n",
      "         -128.0000,   11.1250,   58.7500,  468.0000,  346.0000,   36.0000,\n",
      "            1.3672, -212.0000, -129.0000,  -20.2500, -256.0000,  548.0000,\n",
      "           36.0000,  159.0000]], dtype=torch.bfloat16)\n",
      "\n",
      "Without bias:\n",
      "\n",
      " tensor([[ 109.5000,  -52.5000, -300.0000,  812.0000,   39.7500,   66.5000,\n",
      "           77.0000,   -6.8125, -164.0000, -123.0000,    8.3125,  -61.7500,\n",
      "          183.0000,  155.0000,    5.2500, -346.0000,  -60.7500,  -13.0625,\n",
      "         -126.0000,    9.9375,   59.2500,  468.0000,  346.0000,   35.7500,\n",
      "            1.6094, -213.0000, -129.0000,  -19.3750, -254.0000,  548.0000,\n",
      "           35.0000,  159.0000]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(\"With bias:\\n\\n\", \n",
    "      w8_a16_forward(w_int8, hs, scales, bias))\n",
    "\n",
    "print(\"\\nWithout bias:\\n\\n\", \n",
    "      w8_a16_forward(w_int8, hs, scales))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "height": 31
   },
   "source": [
    "### `W8A16LinearLayer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "height": 117
   },
   "source": [
    "- This is `init` signature of [PyTorch Linear layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear):\n",
    "```Python\n",
    "def __init__(self, in_features, out_features, bias=True,\n",
    "             device=None, dtype=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 661
   },
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This will error on layer initialization\n",
    "        # RuntimeError :  Only Tensors of floating point and complex dtype can require gradients \n",
    "        #self.int8_weights = nn.Parameter(torch.Tensor([0, 1]\n",
    "        #                             ).to(dtype=torch.int8))\n",
    "\n",
    "        # This says dont compute gradients on them.       \n",
    "        self.register_buffer(\"int8_weights\",\n",
    "                            torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))        \n",
    "        self.register_buffer(\"scales\", \n",
    "                             torch.randn((out_features), dtype=dtype))        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((1, out_features), \n",
    "                                             dtype=dtype))        \n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def quantize(self, weights):\n",
    "        w_fp32 = weights.clone().to(torch.float32) # for stability\n",
    "        \n",
    "        # Per channel since max from last dim \n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127 \n",
    "        scales = scales.to(weights.dtype) # same type as weights\n",
    "        \n",
    "        # linear quantization\n",
    "        int8_weights = torch.round(weights/scales.unsqueeze(1)).to(torch.int8) \n",
    "        self.int8_weights = int8_weights\n",
    "        self.scales = scales\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, \n",
    "                              input, self.scales, self.bias)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 167
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 32]), torch.float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dtypes\n",
    "module = W8A16LinearLayer(16, 32) # input_hidden_size, output_hidden_size\n",
    "print(module.int8_weights.shape)\n",
    "print(module.scales.shape)\n",
    "\n",
    "# Check dtypes\n",
    "dummy_hidden_states = torch.randn(1, 6, 16) # bs, seq_len, input_hidden_size\n",
    "dummy_output_states = module(dummy_hidden_states)\n",
    "dummy_output_states.shape, dummy_output_states.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 65
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3539, -0.5164,  0.7794,  1.2950, -1.1936,  1.1703, -1.0569,  1.5416]) torch.Size([8])\n",
      "Weights at init:\n",
      " tensor([[ -98,   59,   44,   21],\n",
      "        [  42,   58,   54, -118],\n",
      "        [  26,   37, -117,  -53],\n",
      "        [ -92, -127,  -28,  111],\n",
      "        [ -89,   92,  -51,  -11],\n",
      "        [  32,   99,  101,   69],\n",
      "        [  32, -127,   24,   42],\n",
      "        [ -44, -107,  -62,  108]], dtype=torch.int8) torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "module = W8A16LinearLayer(4, 8)\n",
    "print(module.scales, module.scales.shape)\n",
    "print(\"Weights at init:\\n\" , module.int8_weights, module.int8_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 48
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig Weights:\n",
      " tensor([[-0.0693,  0.1245, -1.1562,  0.8398,  1.3828, -0.1406,  0.1406, -0.2793],\n",
      "        [-0.7891,  0.0123, -1.2266, -0.6914, -0.4023,  0.2441, -0.4766,  0.1001],\n",
      "        [ 0.7812, -0.6797, -0.2695, -1.0156, -2.5781, -0.5312,  0.1348, -2.2344],\n",
      "        [-0.6875, -2.4062, -0.8516, -0.9727, -1.1484,  0.5781, -0.8242, -0.1738]],\n",
      "       dtype=torch.bfloat16) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn((4, 8), dtype=torch.bfloat16)\n",
    "print(\"Orig Weights:\\n\" , w, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 48
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quant Weights:\n",
      " tensor([[  -6,   11, -106,   78, -128,  -13,   13,  -26],\n",
      "        [ -82,    1, -127,  -72,  -42,   25,  -50,   10],\n",
      "        [  38,  -34,  -13,  -50, -127,  -26,    7, -110],\n",
      "        [ -36, -127,  -45,  -52,  -61,   30,  -44,   -9]], dtype=torch.int8) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "module.quantize(w)\n",
    "print(\"Quant Weights:\\n\" , module.int8_weights,  module.int8_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 65
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dequant Weights:\n",
      " tensor([[-0.0654,  0.1196, -1.1484,  0.8477, -1.3906, -0.1416,  0.1416, -0.2832],\n",
      "        [-0.7891,  0.0096, -1.2266, -0.6953, -0.4043,  0.2412, -0.4824,  0.0967],\n",
      "        [ 0.7695, -0.6875, -0.2637, -1.0156, -2.5781, -0.5273,  0.1416, -2.2344],\n",
      "        [-0.6797, -2.4062, -0.8516, -0.9844, -1.1562,  0.5664, -0.8320, -0.1699]],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "### dequantized weights\n",
    "wdeq = module.int8_weights * module.scales.unsqueeze(1)\n",
    "print(\"Dequant Weights:\\n\",wdeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "height": 48
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0913, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantization error\n",
    "(w - wdeq).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Linear layers in Pytorch module + Quantization of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "height": 388
   },
   "outputs": [],
   "source": [
    "def replace_linear_with_target_and_quantize(module, \n",
    "                               target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not \\\n",
    "        any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            old_weight = child.weight\n",
    "\n",
    "            new_module = target_class(child.in_features, \n",
    "                                      child.out_features, \n",
    "                                      old_bias is not None, \n",
    "                                      child.weight.dtype)\n",
    "            setattr(module, name, new_module)\n",
    "\n",
    "            getattr(module, name).quantize(old_weight)\n",
    "            \n",
    "            if old_bias is not None:\n",
    "              getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            # Recursively call the function for nested modules\n",
    "            replace_linear_with_target_and_quantize(child, \n",
    "                     target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "height": 184
   },
   "outputs": [],
   "source": [
    "class DummyModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.emb = torch.nn.Embedding(1, 1)\n",
    "    # Try with bias\n",
    "    self.linear_1 = nn.Linear(1, 1)\n",
    "    # Try without bias\n",
    "    self.linear_2 = nn.Linear(1, 1, bias=False)\n",
    "    # Lm prediction head\n",
    "    self.lm_head = nn.Linear(1, 1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "height": 71
   },
   "outputs": [],
   "source": [
    "model_1 = DummyModel()\n",
    "model_2 = DummyModel()\n",
    "model_3 = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "height": 48
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_target_and_quantize(model_1, W8A16LinearLayer, [\"lm_head\"])\n",
    "print(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "height": 48
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): W8A16LinearLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_target_and_quantize(model_2, W8A16LinearLayer, [])\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "height": 48
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_target_and_quantize(model_3, W8A16LinearLayer, [\"lm_head\"])\n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18JkbYYUQIu0"
   },
   "source": [
    "### Test the Implementation on Various LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJXWk2iYAq2H"
   },
   "source": [
    "- Text generation model: [Salesforce/codegen-350M-mono](https://huggingface.co/Salesforce/codegen-350M-mono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28088,
     "status": "ok",
     "timestamp": 1700735329634,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 150,
    "id": "M01rWujxQO2Y",
    "outputId": "54856450-faed-4480-ba46-591506dcd7bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"./models/Salesforce/codegen-350M-mono\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                    torch_dtype=torch.bfloat16, \n",
    "                                             low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "height": 31,
    "id": "c7cd8ykzBR1m"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'def hello_world():\\n    print(\"Hello World\")\\n\\nhello_world()\\n\\n# 파'}]\n"
     ]
    }
   ],
   "source": [
    "print(pipe(\"def hello_world():\", max_new_tokens=20, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1700735330300,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 31,
    "id": "l66Ugwk_j9ad",
    "outputId": "91cfa736-1bfe-41eb-b9a6-49195836a585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model before:\n",
      "\n",
      " CodeGenForCausalLM(\n",
      "  (transformer): CodeGenModel(\n",
      "    (wte): Embedding(51200, 1024)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-19): 20 x CodeGenBlock(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CodeGenAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (mlp): CodeGenMLP(\n",
      "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Model before:\\n\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 3218,
     "status": "ok",
     "timestamp": 1700735374301,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 63,
    "id": "QXg6Dn5xB051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint of the model in MBs:  797.310976\n"
     ]
    }
   ],
   "source": [
    "previous_memory_footprint = model.get_memory_footprint()\n",
    "print(\"Footprint of the model in MBs: \", \n",
    "      previous_memory_footprint/1e+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 7209,
     "status": "ok",
     "timestamp": 1700735337508,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 48,
    "id": "tJp5UjGdj_EY"
   },
   "outputs": [],
   "source": [
    "replace_linear_with_target_and_quantize(model, \n",
    "                                        W8A16LinearLayer, [\"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1700735337508,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 31,
    "id": "IRIJbF07kJj3",
    "outputId": "b76b4042-1485-451d-cc2d-4952210e2874"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeGenForCausalLM(\n",
       "  (transformer): CodeGenModel(\n",
       "    (wte): Embedding(51200, 1024)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-19): 20 x CodeGenBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CodeGenAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (qkv_proj): W8A16LinearLayer()\n",
       "          (out_proj): W8A16LinearLayer()\n",
       "        )\n",
       "        (mlp): CodeGenMLP(\n",
       "          (fc_in): W8A16LinearLayer()\n",
       "          (fc_out): W8A16LinearLayer()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20139,
     "status": "ok",
     "timestamp": 1700735357638,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 48,
    "id": "03iimdsbRICt",
    "outputId": "b9ac2eed-5e96-457f-c4e0-6d84d5605f61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world():\n",
      "    print(\"Hello World\")\n",
      "\n",
      "# hello_world()\n",
      "\n",
      "# def hello_\n"
     ]
    }
   ],
   "source": [
    "print(pipe(\"def hello_world():\", max_new_tokens=20, \n",
    "           do_sample=False)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "height": 65
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint of the model in MBs:  546.021376\n"
     ]
    }
   ],
   "source": [
    "new_footprint = model.get_memory_footprint()\n",
    "print(\"Footprint of the model in MBs: \", \n",
    "      new_footprint/1e+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "height": 70
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saved in MBs:  251.2896\n"
     ]
    }
   ],
   "source": [
    "### Memory saved\n",
    "print(\"Memory saved in MBs: \", \n",
    "      (previous_memory_footprint - new_footprint)/1e+6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_l5R7AFoVpd"
   },
   "source": [
    "## Memory Efficient Model Loading using `meta`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_l5R7AFoVpd"
   },
   "source": [
    "- First the weights are quantized & saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "5a7754d36b674876a0bae29b4c4f11d9",
      "c658d9df748b48c1adb06e4d3d4902f1",
      "04ca325e4a49419fbee7dd59c0f07fc0",
      "d4c13f7416454241a528c248fdee5a95",
      "44389f5e666e49e089123389d2579ec9",
      "bc39279e834a4afcbff01f5df265dbfb",
      "2eb5f8e58f3744c98e9b89d85410c635",
      "2c03277df8874208a7104394783a18d5",
      "07bd53fa9fcd4314babc5d817934eaef",
      "ae2b9066c32848039d3455a80099527d",
      "97b7eed82a304e23beedd5af1768a56e",
      "6295e65e21854dfa87a9fef743f59766",
      "5ec35cb8548241929be54d7b47939cfd",
      "27489d707ce64af99fc14e064f3c4d06",
      "f6b94c8a65004a6ba8e2761d54166c55",
      "52192bd284c944ecb4c03c792f46b369",
      "08535d87616a4748aa6977252cea83f6",
      "5b427337b23447ef8e2d035b1ea375ab",
      "a53e8b3f6e664da297f62184cbc7b62e",
      "0508129e3c254387ad852a136c4b26b8",
      "344ba2e021b84144b39ae5c302c9230a",
      "601ecaebbc184783ae790341d4df9871",
      "a981a5036fb74718a93cea81376bb99b",
      "47d09961cd5945dd8f26562b68f5ad9e",
      "4935b71b9366450594aca81fe3253120",
      "6cb925d5314d477a8de49cdbfbe98df0",
      "8686aa3f09f949e89335788c9dd2c021",
      "a61db1587b44416cb8d25cdf7b1f30f8",
      "021bb3b96943486d9460163e9a2ed3fc",
      "9102b226914b467e99295ffc4e29930b",
      "7d4278ef0ac6415689acfbe9f9b5c7d2",
      "8c914a5f84584e74bd68284a15f2861c"
     ]
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1700734507308,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 133,
    "id": "BeSUMWv5rOLl",
    "outputId": "81c3def4-3c3a-4ad6-d827-5a05230fe440"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"./models/facebook/opt-125m\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 1198,
     "status": "ok",
     "timestamp": 1700735397916,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 65,
    "id": "WKHiXdnUoVQb"
   },
   "outputs": [],
   "source": [
    "replace_linear_with_target_and_quantize(model, \n",
    "                             W8A16LinearLayer, \n",
    "                                   [\"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 2075,
     "status": "ok",
     "timestamp": 1700735400948,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 31,
    "id": "w6MNxYD3rmGh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): W8A16LinearLayer()\n",
       "            (v_proj): W8A16LinearLayer()\n",
       "            (q_proj): W8A16LinearLayer()\n",
       "            (out_proj): W8A16LinearLayer()\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): W8A16LinearLayer()\n",
       "          (fc2): W8A16LinearLayer()\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1700735401224,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 48,
    "id": "xXoiKTZErq-o"
   },
   "outputs": [],
   "source": [
    "quantized_state_dict = model.state_dict()\n",
    "torch.save(quantized_state_dict, \"quantized_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optionally push them to Huggingface hub\n",
    "\n",
    "```Python\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "YOUR_HF_USERNAME = \"\"\n",
    "your_repo_id = f\"{YOUR_HF_USERNAME}/opt-125m-quantized-dlai\"\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# create_repo(your_repo_id)\n",
    "\n",
    "api.upload_file(\n",
    " path_or_fileobj=\"quantized_state_dict.pth\",\n",
    " path_in_repo=\"quantized_state_dict.pth\",\n",
    " repo_id=your_repo_id\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIsqzrFDse7I"
   },
   "source": [
    "- When you load quantized weights next time, use `meta` device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1700735433204,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 200,
    "id": "qL8wID_ysyVD"
   },
   "outputs": [],
   "source": [
    "from transformers import OPTForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "model_id = \"./models/facebook/opt-125m\"\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "# Loads only skeleton of the model. Weights are not loaded\n",
    "with torch.device(\"meta\"):\n",
    "  model = OPTForCausalLM(config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 727,
     "status": "ok",
     "timestamp": 1700735010719,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 48,
    "id": "YgkfalAts3TM",
    "outputId": "3b0cc8a6-4421-40f0-e1e0-9a03ec322530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(50272, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(2050, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights are not loaded yet\n",
    "for param in model.parameters():\n",
    "  print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "height": 31,
    "id": "tS5P5b7ptPQt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "height": 354
   },
   "outputs": [],
   "source": [
    "def replace_linear_with_target(module, \n",
    "                               target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not \\\n",
    "        any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            old_weight = child.weight\n",
    "\n",
    "            new_module = target_class(child.in_features, \n",
    "                                      child.out_features, \n",
    "                                      old_bias is not None, \n",
    "                                      child.weight.dtype)\n",
    "            setattr(module, name, new_module)\n",
    "            \n",
    "            if old_bias is not None:\n",
    "              getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            # Recursively call the function for nested modules\n",
    "            replace_linear_with_target(child, \n",
    "                     target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1700735436517,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 62,
    "id": "Nu1gi0bUtOSw"
   },
   "outputs": [],
   "source": [
    "# Update the skeleton\n",
    "replace_linear_with_target(model, W8A16LinearLayer, [\"lm_head\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1700735438474,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 31,
    "id": "zW-QYLm-tetF",
    "outputId": "9b19aa34-44fa-436b-fca1-edb47307f374"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): W8A16LinearLayer()\n",
       "            (v_proj): W8A16LinearLayer()\n",
       "            (q_proj): W8A16LinearLayer()\n",
       "            (out_proj): W8A16LinearLayer()\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): W8A16LinearLayer()\n",
       "          (fc2): W8A16LinearLayer()\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1700735439874,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 186,
    "id": "67U3w4pDtjm_"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Quantized weights from huggingface\n",
    "state_dict_cache_path = hf_hub_download(\n",
    "    \"ybelkada/opt-125m-quantized-dlai\",\n",
    "    \"quantized_state_dict.pth\"\n",
    ")\n",
    "\n",
    "# 125MB parameter model => 166MB size\n",
    "# 8bit (1byte) => 125MB + 16bit scales etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "height": 28
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(state_dict_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1085,
     "status": "ok",
     "timestamp": 1700735443407,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 61,
    "id": "uOp6TKy4t0hQ",
    "outputId": "2f35af16-fbcc-4f26-cea9-e5af43fd0532"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads quantized weights into the model\n",
    "model.load_state_dict(state_dict, strict=True, assign=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12352,
     "status": "ok",
     "timestamp": 1700735485242,
     "user": {
      "displayName": "Younes Belkada",
      "userId": "15414910276690549281"
     },
     "user_tz": -60
    },
    "height": 82,
    "id": "SgLYmERGu2xu",
    "outputId": "e7200afe-9108-4b34-874b-2fa85d7f2805"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello today I am a new member of the team.\\nI am a new member of the team.\\nI am a new member of the team.\\nI am a new member of the team.\\nI am'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "pipe(\"Hello today I am\", max_new_tokens=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "height": 48
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello today I am giving a course about the new technology of the future.\\nI am'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "pipe(\"Hello today I am giving a course about\", max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NgmjMISuIyzF",
    "3dMVgqcNJCqE",
    "MFx2m7RmzRd5",
    "l6XbkmOzYMrC",
    "8NS1TnQt6E6v"
   ],
   "provenance": [
    {
     "file_id": "12_pQW6LB80u98m72_YKwM4ph7eb5Uf3g",
     "timestamp": 1705360205453
    },
    {
     "file_id": "1U9pm4j_uAD8EO7OrEPvdpwFjQKO3suuH",
     "timestamp": 1700237940920
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
