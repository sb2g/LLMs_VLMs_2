{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6tCv-A_bJVb"
   },
   "source": [
    "## Reference:\n",
    "- https://learn.deeplearning.ai/courses/quantization-fundamentals/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6tCv-A_bJVb"
   },
   "source": [
    "# Overview: 8-bit Linear quantization\n",
    "\n",
    "This notebook shows how to perform 8-bit Linear Quantization. using Huggingface `quanto` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 82
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53402cf037b41b1aa7f98fc4a55f95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc95a1322914d2abf8e6afbea51e61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f98b8e279b94365a513791346daa882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b77cab6cf34723ad0134322ef70faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87f81b73f1045c3b9fd2edf6e0b2274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffaa1319b6e4abdbac5854b5ecc8d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0899ff6bf7214dd095215ac53078e3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 99
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> annie scott</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, my name is \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 65
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 0.307844608 GB\n"
     ]
    }
   ],
   "source": [
    "from helper import compute_module_sizes\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model (8-bit precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 48
   },
   "outputs": [],
   "source": [
    "from quanto import quantize, freeze\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "quantize(model, weights=torch.int8, activations=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): QLinear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the model\n",
    "- This step takes a bit of memory, and so for the Pythia model that is shown in the lecture video, it will not run in the classroom.\n",
    "- This will work fine with the smaller T5-Flan model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 48
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 0.12682868 GB\n"
     ]
    }
   ],
   "source": [
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try running inference on the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 167
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> annie scott</s>\n",
      "annie scott\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello, my name is \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "# outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Pythia model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from transformers import AutoModelForCausalLM\n",
    "model_name = \"EleutherAI/pythia-410m\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             low_cpu_mem_usage=True)\n",
    "print(model.gpt_neox)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Write a start of a (`text`) sentence which you'd like the model to complete.\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "outputs\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the model's size using the helper function, `compute_module_sizes`.\n",
    "\n",
    "```Python\n",
    "from helper import compute_module_sizes\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)\n",
    "```\n",
    "- **Note:** The weights are in `fp32`.\n",
    "\n",
    "```Python\n",
    "from quanto import quantize, freeze\n",
    "import torch\n",
    "\n",
    "quantize(model, weights=torch.int8, activations=None)\n",
    "# after performing quantization\n",
    "print(model.gpt_neox)\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)\n",
    "\n",
    "# freeze the model - requires more than 8GB memory \n",
    "freeze(model)\n",
    "print(model.gpt_neox.layers[0].attention.dense.weight)\n",
    "\n",
    "# get model size after quantization\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")\n",
    "\n",
    "# run inference after quantizing the model\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing \"linear quantization\" to \"downcasting\"\n",
    "\n",
    "To recap the difference between the \"linear quantization\" with the \"downcasting\" method:\n",
    "\n",
    "- When downcasting a model, you convert the model's parameters to a more compact data type (bfloat16).  During inference, the model performs its calculations in this data type, and its activations are in this data type.  Downcasting may work with the bfloat16 data type, but the model performance will likely degrade with any smaller data type, and won't work if you convert to an integer data type (like the int8 in this lesson).\n",
    "\n",
    "\n",
    "- \"Linear quantization\" enables the quantized model to maintain performance much closer to the original model by converting from the compressed data type back to the original FP32 data type during inference. So when the model makes a prediction, it is performing the matrix multiplications in FP32, and the activations are in FP32.  This enables you to quantize the model in data types smaller than bfloat16, such as int8, in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpTGuZzDRr5A8ocSoYIFkP",
   "collapsed_sections": [
    "4V_amrl-xG9D",
    "dODA6rR0z297"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
